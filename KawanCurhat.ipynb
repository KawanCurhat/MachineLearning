{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR5aZi97ewie",
        "outputId": "8455e87a-7b8b-4390-9e21-b05ea583a06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "import pickle\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dTnI31eNfk71"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.optimizers import SGD\n",
        "import random\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7Qk870yUgB46"
      },
      "outputs": [],
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open(\"intents.json\").read()\n",
        "intents = json.loads(data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "weKR7UILih0A"
      },
      "outputs": [],
      "source": [
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    words.extend(w)\n",
        "    documents.append((w, intent['tag']))\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AomKlhmTj1nv",
        "outputId": "e9f40e1f-3f02-4ef7-e15b-a5bf13646bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~This is words list~\n",
            "['ada', 'orang']\n",
            "--------------------------------------------------\n",
            "~This is documents list~\n",
            "[(['Hi', 'there'], 'greetings'), (['Hello'], 'greetings')]\n",
            "--------------------------------------------------\n",
            "~This is classes list~\n",
            "['evening', 'night', 'goodbye', 'thanks', 'no-response', 'neutral-response', 'about']\n"
          ]
        }
      ],
      "source": [
        "print(\"~This is words list~\")\n",
        "print(words[3:5])\n",
        "print(\"-\"*50)\n",
        "print(\"~This is documents list~\")\n",
        "print(documents[3:5])\n",
        "print(\"-\"*50)\n",
        "print(\"~This is classes list~\")\n",
        "print(classes[3:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiiaPqWwkVrL",
        "outputId": "b24e567e-4391-4b55-d097-2d4a7682eabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~Document Length~\n",
            "310 documents\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "~Class Length~\n",
            "113 classes\n",
            "\n",
            " ['about', 'afternoon', 'anxious', 'ask', 'casual', 'creation', 'death', 'default', 'depressed', 'dialog.holdon', 'dialog.hug', 'dialog.idontcare', 'dialog.introduction', 'dialog.sorry', 'done', 'evening', 'fact-1', 'fact-10', 'fact-11', 'fact-12', 'fact-13', 'fact-14', 'fact-15', 'fact-16', 'fact-17', 'fact-18', 'fact-19', 'fact-2', 'fact-20', 'fact-21', 'fact-22', 'fact-23', 'fact-24', 'fact-25', 'fact-26', 'fact-27', 'fact-28', 'fact-29', 'fact-3', 'fact-30', 'fact-31', 'fact-32', 'fact-5', 'fact-6', 'fact-7', 'fact-8', 'fact-9', 'friends', 'goodbye', 'greetings', 'greetings.bye', 'greetings.day', 'greetings.hello', 'greetings.howareyou', 'greetings.islam', 'greetings.nicetomeetyou', 'greetings.nicetoseeyou', 'greetings.nicetotalktoyou', 'greetings.night', 'greetings.noon', 'greetings.whatareyoudoing', 'happy', 'hate-me', 'hate-you', 'help', 'jokes', 'learn-mental-health', 'learn-more', 'location', 'meditation', 'mental-health-fact', 'morning', 'motivation.borkenheart', 'motivation.giveup', 'motivation.nevergiveup', 'motivation.whatis.bestfriend', 'motivation.whatis.boy/girl.friend', 'motivation.whatisitfriend', 'name', 'neutral-response', 'night', 'no-approach', 'no-response', 'not-talking', 'problem', 'repeat', 'rhea-useful', 'sad', 'scared', 'skill', 'sleep', 'something-else', 'stressed', 'stupid', 'suicide', 'thanks', 'understand', 'user-advice', 'user-agree', 'user-meditation', 'user.angry', 'user.back', 'user.bored', 'user.busy', 'user.cannotsleep', 'user.excited', 'user.likeagent', 'user.lovesagent', 'user.needsadvice', 'user.permission_to_ask', 'user.testing', 'worthless', 'wrong']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "~Word Length~\n",
            "361 unique lemmatized words\n",
            "\n",
            " ['%', ',', '.', '[', ']', 'a', 'ada', 'adalah', 'adikku', 'ahli', 'akal', 'akan', 'aku', 'alami', 'anak', 'anda', 'antara', 'apa', 'apa-apa', 'apakah', 'artinya', 'assalamualaikum', 'atas', 'atau', 'au', 'ayahku', 'bagaimana', 'bagi', 'bahagia', 'baik', 'baik-baik', 'bantu', 'bantuan', 'bantuannya', 'banyak', 'baru', 'beban', 'beberapa', 'begitu', 'bekerja', 'belajar', 'belum', 'benar', 'benci', 'berbicara|mengobrol', 'berguna', 'berharga', 'berlatih', 'berpelukan', 'berpikir', 'bersemangatnya', 'bersemangat|senang', 'bertanya', 'bertemu', 'betapa', 'biasa', 'bicarakan', 'bisa', 'bisakah', 'bodoh', 'boleh', 'bolehkah', 'bonjour', 'bosan', 'bunuh', 'butuh', 'cemas', 'ceria', 'ceritakan', 'chatbot', 'cinta', 'cukup', 'dalam', 'dan', 'dapat', 'dapatkah', 'definisikan', 'dekat', 'dengan', 'denganmu', 'denganmu|melihatmu', 'depresi', 'di', 'diam', 'diciptakan', 'dikatakan', 'dilakukan', 'diri', 'dirimu', 'do', 'doe', 'dukung', 'dukungan', 'dunia', 'fakta', 'fokus', 'gangguan', 'gejala', 'gila', 'guten', 'hai', 'hal', 'halo', 'halo|hai|helo', 'hanya', 'hanyalah', 'harap', 'hari', 'harimu|hari', 'harus', 'hati', 'hati-hati', 'hello', 'hey', 'hi', 'hmmm', 'hola', 'howdy', 'hubungan', 'ibuku', 'ingin', 'ini', 'insomnia', 'insomnia|tidak', 'is', 'istirahat', 'itu', 'jalan', 'jauh', 'jawaban', 'jenis', 'jika', 'jiwa', 'juga', 'jumpa', 'k', 'kabar', 'kakakku', 'kamu', 'kamu|', 'karena', 'kasih', 'katakan', 'kau', 'kecemasan', 'kedengarannya', 'kelebihan', 'kelompok', 'keluarga', 'keluargaku', 'kembali', 'kembali|di', 'kendalikan', 'kesedihan', 'kesehatan', 'kesepian', 'ketahui', 'keuangan', 'kita', 'konnichiwa', 'kosong', 'kupikirkan', 'kurasa', 'lagi', 'lain', 'lakukan', 'lanjut', 'lebih', 'lelah', 'lelucon', 'lewat', 'lokasi', 'luar', 'maaf', 'maafkan', 'malam', 'mana', 'mandek', 'marah', 'mari', 'masalah', 'masih', 'masuk', 'mati', 'mau', 'meditasi', 'melakukan', 'memanggilmu', 'membantu', 'membenciku', 'memberi', 'memberitahuku', 'membicarakan', 'membicarakannya', 'membosankan', 'membuatku', 'membuka', 'memelukmu', 'memiliki', 'mempelajarinya', 'memulai', 'mencari', 'mencegah', 'mencintaimu', 'menciptakan', 'mendapatkan', 'menderita', 'menemukan', 'mengalami', 'mengapa', 'mengenal', 'mengerikan', 'mengerti', 'mengkhawatirkan', 'mengobrol', 'mengulanginya', 'meninggal', 'menjaga', 'menjauhlah', 'mental', 'menunggu', 'menyebabkan', 'menyebutkannya', 'menyerah', 'menyesal', 'menyukai', 'menyukaiku', 'menyukaimu', 'merasa', 'minta', 'mu', 'mulai', 'mungkin', 'mu|kabarmu|kabar', 'nama', 'namamu', 'name', 'nanya', 'nasihat', 'nyenyak', 'obat', 'oh', 'ok', 'oke', 'ola', 'orang', 'pacar', 'pada', 'padamu', 'pagi', 'pantas', 'peduli', 'pekerjaan', 'peluk', 'pelukan', 'pendukung', 'pengobatan', 'penting', 'pentingnya', 'penyakit', 'perawatan', 'perbedaan', 'percaya', 'pergi', 'peringatan', 'perkenalkan', 'perlu', 'pikir', 'pilihan', 'profesional', 'punya', 'revoir', 'robot', 'sahabat', 'saja', 'sakit', 'salah', 'sama', 'sampai', 'sana', 'sangat', 'sanggup', 'saya', 'saya|', 'sayonara', 'sebelum', 'sebentar', 'sedang', 'sedih', 'seharusnya', 'sehat', 'sekali', 'sekarang', 'selama', 'selamat', 'selamat|', 'semakin', 'sembuh', 'senang', 'sendiri', 'seperti', 'sepertinya', 'seseorang', 'sesuatu', 'siang', 'siap', 'siapa', 'siapa-siapa', 'sibuk', 'sini|telah', 'sosial', 'spesial', 'stres', 'sudah', 'suka', 'tag', 'tahan', 'tahu', 'takut', 'tampaknya', 'tanda-tanda', 'tanya', 'teman', 'temanku', 'tentang', 'tentu', 'tepat', 'terakhir', 'terapi', 'terima', 'terkena', 'terlibat', 'tersedia', 'terserah', 'tertarik', 'terus', 'test', 'testing', 'therapist', 'there', 'tidak', 'tidur', 'tidur|sulit', 'tinggal', 'tolong', 'tunggu', 'uang', 'ujian', 'ujianku', 'untuk', 'wabarakatu', 'warahmatullahi', 'what', 'ya', 'yang', '|aku', '|ini|itu', '|padamu', '|sangat', '|sedang', '|selamat']\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(\"~Document Length~\")\n",
        "print(len(documents), \"documents\\n\\n\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "print(\"~Class Length~\")\n",
        "print(len(classes), \"classes\\n\\n\", classes)\n",
        "print(\"-\"*100)\n",
        "\n",
        "print(\"~Word Length~\")\n",
        "print(len(words), \"unique lemmatized words\\n\\n\", words)\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7usgobTmJZr",
        "outputId": "404743fa-46cf-4003-a681-949d42482ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data created\n"
          ]
        }
      ],
      "source": [
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "    bag = []\n",
        "    pattern_words = doc[0]\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "\n",
        "\n",
        "print(\"Training data created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJKpSSpmnQfk",
        "outputId": "97aa6075-221a-47e9-a045-18e3ba61163e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 1s 4ms/step - loss: 4.7208 - accuracy: 0.0323\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 4.5747 - accuracy: 0.0677\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 4.3248 - accuracy: 0.0968\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 4.0846 - accuracy: 0.1258\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 3.7701 - accuracy: 0.1903\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 3.4168 - accuracy: 0.2839\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 2.9925 - accuracy: 0.3194\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 2.7676 - accuracy: 0.3839\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 2.4904 - accuracy: 0.4581\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 2.0988 - accuracy: 0.5387\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 1.8953 - accuracy: 0.6032\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 1.6405 - accuracy: 0.6258\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 1.4882 - accuracy: 0.6613\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3881 - accuracy: 0.6871\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.1932 - accuracy: 0.7516\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.0465 - accuracy: 0.7484\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.9353 - accuracy: 0.8065\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.8827 - accuracy: 0.8097\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.8162 - accuracy: 0.7806\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.7144 - accuracy: 0.8129\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6514 - accuracy: 0.8355\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6195 - accuracy: 0.8613\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4921 - accuracy: 0.8903\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4735 - accuracy: 0.8742\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5112 - accuracy: 0.8742\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.9129\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4911 - accuracy: 0.8935\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3742 - accuracy: 0.9129\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.9226\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3460 - accuracy: 0.9194\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.9032\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3015 - accuracy: 0.9355\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.9161\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3133 - accuracy: 0.9258\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2761 - accuracy: 0.9419\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.9548\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2611 - accuracy: 0.9355\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.9355\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.9323\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9484\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.9516\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9677\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9484\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9645\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9581\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9581\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1563 - accuracy: 0.9548\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1332 - accuracy: 0.9742\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1454 - accuracy: 0.9677\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1429 - accuracy: 0.9677\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9581\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1501 - accuracy: 0.9645\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1622 - accuracy: 0.9613\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1485 - accuracy: 0.9548\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1476 - accuracy: 0.9613\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1608 - accuracy: 0.9484\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1349 - accuracy: 0.9677\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.1469 - accuracy: 0.9613\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.1577 - accuracy: 0.9516\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9548\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9645\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9581\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.9774\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1099 - accuracy: 0.9710\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.9742\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9677\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.1304 - accuracy: 0.9710\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.1220 - accuracy: 0.9677\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0943 - accuracy: 0.9806\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0962 - accuracy: 0.9677\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9710\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.1024 - accuracy: 0.9710\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9645\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.1071 - accuracy: 0.9645\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0987 - accuracy: 0.9742\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0885 - accuracy: 0.9839\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.1026 - accuracy: 0.9613\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0860 - accuracy: 0.9774\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.9742\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9677\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9806\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9839\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9806\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1003 - accuracy: 0.9677\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.9645\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 0.9774\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9806\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9774\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9710\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9839\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9742\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9742\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0899 - accuracy: 0.9774\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9774\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9839\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.9742\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9935\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0854 - accuracy: 0.9710\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9710\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9806\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9742\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9774\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9839\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9645\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9742\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9871\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9871\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9806\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9806\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9774\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.9677\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0935 - accuracy: 0.9742\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9903\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9839\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9710\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.9774\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0677 - accuracy: 0.9774\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.9871\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9871\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.0791 - accuracy: 0.9677\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 0.9710\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9806\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9677\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.0491 - accuracy: 0.9806\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9871\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0391 - accuracy: 0.9935\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9839\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9774\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.9871\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 0.9935\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.9839\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0635 - accuracy: 0.9806\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 0.9968\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.0440 - accuracy: 0.9903\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0528 - accuracy: 0.9806\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.9742\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0406 - accuracy: 0.9806\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0425 - accuracy: 0.9871\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 0.9871\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0411 - accuracy: 0.9839\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.9903\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9871\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9774\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9774\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9903\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9774\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.9839\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9774\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9774\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0519 - accuracy: 0.9839\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9806\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.9871\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9839\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 0.9903\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.9774\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9806\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9839\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.9903\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.9806\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.9903\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.9935\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0349 - accuracy: 0.9903\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9806\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.9903\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9903\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.9774\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.9903\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9742\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0363 - accuracy: 0.9903\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.9968\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.9774\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.9871\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.9903\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.9839\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0369 - accuracy: 0.9806\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.9903\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0412 - accuracy: 0.9903\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.9903\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 0.9839\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9839\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.9935\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0384 - accuracy: 0.9935\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0416 - accuracy: 0.9871\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.9871\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9839\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.9839\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0461 - accuracy: 0.9871\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.9935\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9806\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 0.9968\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0493 - accuracy: 0.9871\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 0.9871\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0271 - accuracy: 0.9903\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 0.9839\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9968\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.9839\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0270 - accuracy: 0.9903\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9903\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 0.9806\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.9806\n",
            "\n",
            "\n",
            "**************************************************\n",
            "\n",
            "Model Created Successfully\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "adam_optimizer = Adam()\n",
        "# sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot.h5', hist)\n",
        "print(\"\\n\")\n",
        "print(\"*\" * 50)\n",
        "print(\"\\nModel Created Successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JYQ8z2UyUtlb"
      },
      "outputs": [],
      "source": [
        "model = load_model('chatbot.h5')\n",
        "intents = json.loads(open(\"intents.json\").read())\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MAof1vJGpJjF"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "  return sentence_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sgVX53roKA7G"
      },
      "outputs": [],
      "source": [
        "def bow(sentence, words, show_details=True):\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  bag = [0]*len(words)\n",
        "  for s in sentence_words:\n",
        "    for i, w in enumerate(words):\n",
        "      if w == s:\n",
        "        bag[i] = 1\n",
        "        if show_details:\n",
        "          print(\"found in bag: %s\" %w)\n",
        "  return(np.array(bag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qszkeFIcLDLs"
      },
      "outputs": [],
      "source": [
        "def predict_class(sentence, model):\n",
        "  p = bow(sentence, words, show_details=False)\n",
        "  res = model.predict(np.array([p]))[0]\n",
        "  error = 0.25\n",
        "  results = [[i, r] for i, r in enumerate(res) if r>error]\n",
        "\n",
        "  results.sort(key=lambda x:[1], reverse=True)\n",
        "  return_list= []\n",
        "\n",
        "  for r in results:\n",
        "    return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "  return return_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HVQDyJNzL4kt"
      },
      "outputs": [],
      "source": [
        "def getResponse(ints, intents_json):\n",
        "  tag = ints[0]['intent']\n",
        "  list_of_intents = intents_json['intents']\n",
        "  for i in list_of_intents:\n",
        "    if(i['tag']== tag):\n",
        "      result = random.choice(i['responses'])\n",
        "      break\n",
        "  return result\n",
        "\n",
        "def chatbot_response(text):\n",
        "  ints = predict_class(text, model)\n",
        "  res = getResponse(ints, intents)\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xSZ3pT3LMf7H"
      },
      "outputs": [],
      "source": [
        "def start_chat():\n",
        "  print(\"Bot: Rhea is here! How can help you?.\\n\\n\")\n",
        "  while True:\n",
        "    inp = str(input()).lower()\n",
        "    if inp.lower()==\"end\":\n",
        "      break\n",
        "    if inp.lower()== '' or inp.lower()== '*':\n",
        "      print('Please re-phrase your query!')\n",
        "      print(\"-\"*50)\n",
        "    else:\n",
        "      print(f\"Bot: {chatbot_response(inp)}\"+'\\n')\n",
        "      print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nePtg9TeNnTY",
        "outputId": "bf3d5adf-ef23-4012-c76c-0e48b89705e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Rhea is here! How can help you?.\n",
            "\n",
            "\n",
            "halo\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "Bot: Halo. Katakan padaku bagaimana perasaanmu hari ini?\n",
            "\n",
            "--------------------------------------------------\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "start_chat()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the Keras model\n",
        "model = load_model('chatbot.h5')\n",
        "\n",
        "# Convert the model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model to a file\n",
        "with open('chatbot.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"*\" * 50)\n",
        "print(\"\\nModel Converted to TensorFlow Lite Successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hwv3zchBAr-",
        "outputId": "0e99bcfe-212c-41d5-ad01-c6348466f3de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**************************************************\n",
            "\n",
            "Model Converted to TensorFlow Lite Successfully\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}